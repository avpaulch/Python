{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e4877d",
   "metadata": {},
   "source": [
    "### Question 1: Ensemble Techniques â€“ Bagging vs Boosting\n",
    "**Bagging** trains multiple models independently on random subsets to reduce variance.\n",
    "**Boosting** trains models sequentially to correct errors and reduce bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6950b5e",
   "metadata": {},
   "source": [
    "### Question 2: Random Forest vs Single Decision Tree\n",
    "Random Forest reduces overfitting by averaging multiple decision trees trained on bootstrapped samples.\n",
    "Key hyperparameters: `n_estimators`, `max_features`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb74d4b",
   "metadata": {},
   "source": [
    "### Question 3: Stacking in Ensemble Learning\n",
    "Stacking combines predictions from multiple models using a meta-model to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3ea32",
   "metadata": {},
   "source": [
    "### Question 4: OOB Score in Random Forest\n",
    "Out-of-Bag score evaluates model performance using samples not included in training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013d289",
   "metadata": {},
   "source": [
    "### Question 5: AdaBoost vs Gradient Boosting\n",
    "AdaBoost adjusts weights of misclassified samples; Gradient Boosting minimizes loss using gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c80fc8",
   "metadata": {},
   "source": [
    "### Question 6: CatBoost and Categorical Features\n",
    "CatBoost handles categorical features using ordered target statistics and efficient encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11e9f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging KNN Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Bagging with KNN\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "model = BaggingClassifier(KNeighborsClassifier(), n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Bagging KNN Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d9e857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Question 8: AdaBoost with Decision Tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30bfb72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Question 9: Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041be649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Question 10: Stacking Classifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimators = [\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2cad3-8c21-447d-8332-cecc4ee32d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
